{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhisheksingh1234/DataScience/blob/master/LSTM_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLT1OLdgyNm1",
        "outputId": "52d9beb5-d8d7-4bda-a559-b01e12b83ab9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to '/content/drive' directory\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAcrGALyx7Dy"
      },
      "outputs": [],
      "source": [
        "# Path to the file in Google Drive after mounting\n",
        "import shutil\n",
        "\n",
        "file_path = '/content/drive/My Drive/winemag-data_first150k.csv'\n",
        "with open('data2.txt','w+') as file_w:\n",
        "  with open(file_path, 'r') as file:\n",
        "      line_count = 0\n",
        "      for line in file:\n",
        "          parts = line.split(',')\n",
        "          description = parts[2]\n",
        "          description = description.strip('\"')\n",
        "          file_w.write(description)\n",
        "          file_w.write(\"\\n\")\n",
        "          print(description)\n",
        "\n",
        "          line_count +=1\n",
        "          if line_count >= 1000:\n",
        "            break\n",
        "\n",
        "shutil.copy('data2.txt','/content/drive/My Drive/data3.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmFNuUBhyXA5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def pad_punctuation(text):\n",
        "    # Using regular expression to split text based on punctuation while keeping punctuation as separate words\n",
        "    padded_text = re.sub(r\"([{}])\".format(re.escape(string.punctuation)), r' \\1 ', text)\n",
        "    return padded_text\n",
        "\n",
        "data_list = []\n",
        "\n",
        "with open('/content/drive/My Drive/data3.txt', 'r') as file:\n",
        "      for line in file:\n",
        "      ## Pad Puntation to treat them as a sperate words\n",
        "        print(line)\n",
        "        data_list = file.readlines()\n",
        "\n",
        "text_data = [pad_punctuation(x) for x in data_list]\n",
        "\n",
        "## Convert it into a tensor flow dataset\n",
        "\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(text_data).batch(32).shuffle(1000)\n",
        "\n",
        "## Keras Text Vectorization Layer, convert to lower case, most prevalent\n",
        "## 10000 words trim or pad token to 201 token long.\n",
        "\n",
        "vectorized_layer = layers.TextVectorization(\n",
        "    standardize = 'lower',\n",
        "    max_tokens = 10000,\n",
        "    output_mode = \"int\",\n",
        "    output_sequence_length = 200+1\n",
        ")\n",
        "\n",
        "## Apply Text Vectorized layer to the training data\n",
        "\n",
        "vectorized_layer.adapt(text_ds)\n",
        "\n",
        "## Vocab variables stores a list of the word tokens\n",
        "vocab = vectorized_layer.get_vocabulary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JY1sGV_E5UTf"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def prepare_inputs(text):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  tokenized_sentences = vectorized_layer(text)\n",
        "  x = tokenized_sentences[:, :-1]\n",
        "  y = tokenized_sentences[:, 1:]\n",
        "  return x,y\n",
        "\n",
        "train_ds = text_ds.map(prepare_inputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDF-A7AqtQad"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "inputs = layers.Input(shape=(None,), dtype = \"int32\")\n",
        "## Embedding Layer Size of Vocublary and dimensionality of the embedding vector\n",
        "x = layers.Embedding(10000,100)(inputs)\n",
        "\n",
        "x = layers.LSTM(128,return_sequences=True)(x)\n",
        "\n",
        "##Dense Layer transforms the hidden states at each timestamp into vector of probabilities for each token\n",
        "outputs = layers.Dense(10000, activation = 'softmax')(x)\n",
        "lstm = Model(inputs,outputs)\n",
        "\n",
        "loss_fn = SparseCategoricalCrossentropy()\n",
        "lstm.compile(\"adam\",loss_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-jBm4dtzeJB",
        "outputId": "221537da-a504-4a2e-9986-43b7d205c873"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "32/32 [==============================] - 76s 2s/step - loss: 0.1543\n",
            "Epoch 2/25\n",
            "32/32 [==============================] - 76s 2s/step - loss: 0.1512\n",
            "Epoch 3/25\n",
            "32/32 [==============================] - 75s 2s/step - loss: 0.1481\n",
            "Epoch 4/25\n",
            "32/32 [==============================] - 75s 2s/step - loss: 0.1458\n",
            "Epoch 5/25\n",
            "32/32 [==============================] - 75s 2s/step - loss: 0.1439\n",
            "Epoch 6/25\n",
            "32/32 [==============================] - 76s 2s/step - loss: 0.1418\n",
            "Epoch 7/25\n",
            "32/32 [==============================] - 75s 2s/step - loss: 0.1388\n",
            "Epoch 8/25\n",
            "32/32 [==============================] - 76s 2s/step - loss: 0.1366\n",
            "Epoch 9/25\n",
            "32/32 [==============================] - 75s 2s/step - loss: 0.1341\n",
            "Epoch 10/25\n",
            "32/32 [==============================] - 76s 2s/step - loss: 0.1328\n",
            "Epoch 11/25\n",
            "32/32 [==============================] - 75s 2s/step - loss: 0.1308\n",
            "Epoch 12/25\n",
            "32/32 [==============================] - 77s 2s/step - loss: 0.1281\n",
            "Epoch 13/25\n",
            "32/32 [==============================] - 75s 2s/step - loss: 0.1262\n",
            "Epoch 14/25\n",
            "32/32 [==============================] - 75s 2s/step - loss: 0.1237\n",
            "Epoch 15/25\n",
            "32/32 [==============================] - 75s 2s/step - loss: 0.1214\n",
            "Epoch 16/25\n",
            "32/32 [==============================] - 75s 2s/step - loss: 0.1197\n",
            "Epoch 17/25\n",
            "32/32 [==============================] - 75s 2s/step - loss: 0.1179\n",
            "Epoch 18/25\n",
            "32/32 [==============================] - 76s 2s/step - loss: 0.1157\n",
            "Epoch 19/25\n",
            "32/32 [==============================] - 75s 2s/step - loss: 0.1142\n",
            "Epoch 20/25\n",
            "32/32 [==============================] - 75s 2s/step - loss: 0.1118\n",
            "Epoch 21/25\n",
            "32/32 [==============================] - 74s 2s/step - loss: 0.1098\n",
            "Epoch 22/25\n",
            "32/32 [==============================] - 72s 2s/step - loss: 0.1082\n",
            "Epoch 23/25\n",
            "32/32 [==============================] - 71s 2s/step - loss: 0.1067\n",
            "Epoch 24/25\n",
            "32/32 [==============================] - 71s 2s/step - loss: 0.1049\n",
            "Epoch 25/25\n",
            "32/32 [==============================] - 72s 2s/step - loss: 0.1034\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Train the model and store the training history\n",
        "history = lstm.fit(train_ds,epochs=25)\n",
        "# Save the trained model\n",
        "model_path = '/content/drive/My Drive/'\n",
        "lstm.save(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "class TextGenerator(callbacks.Callback):\n",
        "    def __init__(self, model, index_to_word, top_k=10):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.index_to_word = index_to_word\n",
        "        self.word_to_index = {word: index for index, word in enumerate(index_to_word)}\n",
        "\n",
        "    def sample_from(self, probs, temperature):\n",
        "        probs = probs ** (1 / temperature)\n",
        "        probs = probs / np.sum(probs)\n",
        "        return np.random.choice(len(probs), p=probs), probs\n",
        "\n",
        "    def generate(self, start_prompt, max_tokens, temperature):\n",
        "        start_tokens = [self.word_to_index.get(x, 1) for x in start_prompt.split()]\n",
        "        sample_token = None\n",
        "        info = []\n",
        "        while len(start_tokens) < max_tokens and sample_token != 0:\n",
        "            x = np.array([start_tokens])\n",
        "            y = self.model.predict(x)\n",
        "            sample_token, probs = self.sample_from(y[0][-1], temperature)\n",
        "            info.append({'prompt': start_prompt, 'word_probs': probs})\n",
        "            start_tokens.append(sample_token)\n",
        "            start_prompt = start_prompt + ' ' + self.index_to_word[sample_token]\n",
        "        print(f\"\\n generated text: \\n {start_prompt}\\n\")\n",
        "        return info\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.generate(\"recipe for\", max_tokens=100, temperature=1.0)"
      ],
      "metadata": {
        "id": "NQVwu_6vUIWD"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = tf.keras.models.load_model('/content/drive/My Drive/')\n",
        "text_gen_callback = TextGenerator(loaded_model, index_to_word)\n",
        "text_gen_callback.on_epoch_end(epoch=0)"
      ],
      "metadata": {
        "id": "XoyfudxdZd8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZU1aHHI-qCM4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQAUv6l1lgvchNDnjYb3DV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}